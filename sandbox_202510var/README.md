論文のアルゴリズム1とアルゴリズム2をよく読むと
エンコードデコードはマルチスケールで繰り返さないことがわかる。

潜在空間fまたはzは加算や内挿ができるということを活用している。

## papaで動かすためにはpapaのcudaドライバーが12.7なのでcuda12.6対応のトーチをインストールしてる

## 質問1

Visual AutoRegressionは潜在空間ベクトルを加算減算と内挿によるダウンサンプリングもアップサンプリングができることを用いてマルチスケールしていますか？

エンコーダーデコーダーの学習時に用いるのはダウンサンプリングもアップサンプリングもしていない元のサイズの画像ですか？

ダウンサンプリングされた潜在空間ベクトルがダウンサンプリングを加えられていない画像で学習されたデコーダーでは解釈できない気がしますが良いのですか？

## 質問2

Visual AutoRegressionでTransformerが学習するトークン列がどのようなものになるのか教えてください
4x4x特徴量(漢字トークン)みたいな例では

年

春夏
秋冬

正月迎春
初夏夏至
暑中秋分
冬至大晦

年春夏秋冬正月迎春初夏夏至暑中秋分冬至大晦というトークン一列が一つの画像を表しているという理解で良いですか

## notebooklm回答2

1. 「年 春夏 秋冬...というトークン一行が一つの画像を表している」
    ◦ これは標準ARモデルの手法であり、VARはこの手法から脱却しています。VARではトークンマップは平坦化されません。
2. 「年春正から年冬晦までのk列のトークンを16行出力している」
    ◦ この記述は、トークンマップを空間的なグリッドとして扱うという点でVARの考え方に近いです。ただし、VARが出力するのは、K個の異なる解像度（スケール）を持つトークンマップのシーケンスです。例えば、最終的な画像解像度（トークンマップのサイズ）が n×n の場合、VARでは、まず最も粗いマップ（例えば 1×1）から始まり、徐々に
	
解像度を上げていき、最終的に n×n のマップに至ります。各ステップ（各スケール）では、そのスケールの全てのトークンが同時に（並列に）予測されます。
したがって、VARのTransformerが学習するトークン列は、粗い解像度のトークンマップから始まり、徐々に細かい解像度のトークンマップへと続く、複数の「マップ」からなるシーケンスであると理解するのが正確です